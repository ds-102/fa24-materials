{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd96f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab04.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2335da1",
   "metadata": {},
   "source": [
    "# Lab 4:  Bayesian Estimation in Hierarchical Graphical Models\n",
    "Welcome to the fourth Data 102 lab! \n",
    "\n",
    "The goal of this lab is to go over Bayesian Estimation and provide an introduction to Hierarchial Graphical Models.\n",
    "\n",
    "The code and responses you need to fill in are represented by `...`. There is additional documentation for each part as you go along. \n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually and do not share your code with anyone other than your partner**. If you do discuss the assignments with people other than your partner please **include their names** in the cell below.\n",
    "\n",
    "You can submit the lab in pairs (groups of two, no more than two). **If you choose to work in a pair, please make sure to add your group member on Gradescope for both Lab 01 written submission and the Lab 01 code submission.**\n",
    "\n",
    "## Submission\n",
    "See the [Gradescope Submission Guidelines post](https://edstem.org/us/courses/52891/discussion/4179226) for details on how to submit your lab.\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, Feburary 14th, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89416bf5",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import beta, binom\n",
    "import itertools\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "\n",
    "import hashlib\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import pymc as pm\n",
    "import logging\n",
    "logger = logging.getLogger('pymc')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8ed97",
   "metadata": {},
   "source": [
    "# Question 1: Beta-Binomial Graphical Model\n",
    "\n",
    "In this question we will look at the COVID modeling example. Here's the summary of what you need to know:\n",
    "\n",
    "In this problem we are trying to estimate the COVID infection risk in households. To do that we curate a list of K studies. Each study has an associated pair $(N_i, X_i)$ where $N_i$ denotes the number of susceptible individuals considered and $X_i$ is the number of them that became infected. In our modeling assumptions we assume that each susceptible person gets infected with probability $\\theta_i$. In epidemiology, this quantity is known as Secondary Attack Rate, or SAR for short.\n",
    "\n",
    "We're trying to do two things: \n",
    "1. We want to *combine* the information from all the studies, so we can get a better estimate of SAR than we would with any individual study on its own. \n",
    "2. We want to understand why the studies got different results: specifically, we'd like to figure out the regions with the *lowest* SAR, so that we can investigate what contributed to their relative success. In the other direction, we want to know which regions had the *highest* SAR, since they're likely the ones most urgently in need of intervention measures to help slow the spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out a dataset \n",
    "study_df = pd.read_csv(\"study_df.csv\", header=0)\n",
    "study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b74e24",
   "metadata": {},
   "source": [
    "## 1a) Compute the naive estimate of SAR\n",
    "\n",
    "\n",
    "The most straightforward way to estimate the probability of infection (SAR) is to divide the number of infected cases by the number of susceptible cases. \n",
    "\n",
    "Compute this quantity in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36549d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the function\n",
    "def trivial_theta_estimate(N_value, X_value):\n",
    "    \"\"\"\n",
    "    Computes the trivial estimate of the Secondary Attack Rate\n",
    "    \n",
    "    Inputs:\n",
    "        N_value : int, number of susceptible individuals\n",
    "        X_value : int, number of infected individuals\n",
    "        \n",
    "    Output:\n",
    "        theta_est : float, estimate of probability of infection (SAR)\n",
    "    \"\"\"\n",
    "    theta_est = ...\n",
    "    return theta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b425ad",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38722ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply  'trivial_theta_estimate' to each row of the dataframe and add a new column to hold the result\n",
    "study_df['Trivial estimate'] = study_df.apply(lambda row: trivial_theta_estimate(row['N'], row['X']), axis =1)\n",
    "study_df.sort_values('Trivial estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676667d",
   "metadata": {},
   "source": [
    "**Trivial estimates suggest that both minimum and maximum probabilities of infection correspond to small studies.**\n",
    "\n",
    "|      | Min     | Max     |\n",
    "|------|---------|---------|\n",
    "| Name | Study 1 | Study 2 |\n",
    "| X    | 2       | 8       | \n",
    "| N    | 11      | 12      |\n",
    "|$\\theta$| 0.18  | 0.50    |\n",
    "\n",
    "\n",
    "Intuitively, we probably shouldn't be making policy decision based on such small studies alone, especially when this dataset has other studies with tens or even hundreds of people. We would like to balance between strong evidence from the small studies and high confidence in estimates from larger studies.\n",
    "\n",
    "Bayesian inference provides a flexible framework to balance our a priori beliefs with new evidence. Consider the following graphical model:\n",
    "\n",
    "\n",
    "![](model.png)\n",
    "\n",
    "\n",
    "The circles represent random variables, and shaded circles represent observed random variables. The diamond at the top represents fixed, unknown parameters . You'll also see people draw dots or squares for these: there isn't really one consistent notation.\n",
    "\n",
    "Here are a few important quantities in Bayesian inference. This lingo will be used at length in this course and in anything you'll learn in the future about Bayesian inference, so make sure you get familiar with it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ae64d",
   "metadata": {},
   "source": [
    "### Joint Density / Joint Distribution:\n",
    "The structure of the graphical model specified the full joint density of the parameters and data in the model. For this example the joint density is:\n",
    "$$p(\\theta_1, \\theta_2, \\ldots, \\theta_K, X_1, \\ldots, X_K) = \\prod_{\\text{vertex $V$ in graph}}p(V|\\text{parent of $V$}) = \\prod_{i=1}^K \\underbrace{p(\\theta_i|\\alpha, \\beta)}_{\\text{prior of $\\theta_i$}} \\prod_{i=1}^K \\underbrace{p(X_i|\\theta_i)}_{\\text{likelihood of data $X_i$}}$$\n",
    "\n",
    "The factorization of the joint density into products of priors and likelihoods is the key feature of Hierarchical Models. It allows to take a complex $2K$-dimensional joint probability and factorize it into products of 1-dimensional probabilities. This factorization is useful because it lets us simplify the distribution and control the amount of computation we have to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6207f8f",
   "metadata": {},
   "source": [
    "### Prior:  $\\theta_i \\sim {\\rm Beta}(\\alpha, \\beta)$\n",
    "\n",
    "We have the prior distribution:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta_i) \n",
    "    &= \\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\\\\n",
    "    &\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "where $\\Gamma$ is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function). Since $\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}$ does not depend on the value of $\\theta$. It is a scaling factor that ensures that $p(\\theta_i)$ is a valid probability function. This leads to a common notation in practice: $p(\\theta_i)\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}$. The symbol $\\propto_{\\theta_i}$ means proportional in $\\theta_i$. This is a little more explicit than the $\\propto$ notation that you usually see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b796a8",
   "metadata": {},
   "source": [
    "### Likelihood: $X_i|\\theta \\sim {\\rm Binomial}(N_i, \\theta_i)$\n",
    "\n",
    "We'll use the notation $p(X_i|\\theta)$ for the likelihood function, which represents our belief about the distribution of the data if we know what the parameter $\\theta$ is (in other words, if we condition on $\\theta$).\n",
    "$$p(X_i|\\theta_i) = \\binom{N_i}{X_i} \\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1c61f",
   "metadata": {},
   "source": [
    "### Marginal: Unconditional distribution of $X_i$:\n",
    "\n",
    "\\begin{align}\n",
    "p(X_i)\n",
    "    &= \\int_{\\theta_i} \\overbrace{p(X_i, \\theta_i)}^{\\text{joint distribution}} \\\\\n",
    "    &= \\int_0^1 \\underbrace{p(X_i|\\theta_i)}_{\\text{likelihood}} \\,\\underbrace{p(\\theta_i)}_{\\text{prior}} \\,d\\theta_i\n",
    "\\end{align}\n",
    "\n",
    "This is the marginal distribution over the data: we can plug in a particular set of $X_i$ values and get out the probability that our model assigns to those values, averaged over all possible values of $\\theta$.\n",
    "\n",
    "When formulating a model, we usually choose the prior and the likelihood based on what we know about the problem. This means that computing this marginal distribution over $X_i$ requires *marginalizing* over the parameter $\\theta$: that involves either a summation or an integral (in this case it's an integral because $\\theta$ is continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a612a",
   "metadata": {},
   "source": [
    "### Posterior: $\\theta_i|X_i$\n",
    "The goal of many estimation problems is to obtain the posterior distribution of the parameter of interest $\\theta_i$ conditioned on the data $X_i$.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta_i|X_i) &= \\frac{p(X_i|\\theta_i)p(\\theta_i)}{p(X_i)} \\quad \\text{(by Bayes Rule)}\\\\\n",
    "&\\propto_{\\theta} p(X_i|\\theta_i)p(\\theta_i) \\quad \\text{(the data marginal $p(X_i)$ does not depend on $\\theta$)}\\\\\n",
    "&\\propto_{\\theta}  \\underbrace{\\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}}_{\\text{likelihood}} \\underbrace{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}_{\\text{prior}}\\\\\n",
    "&\\propto_{\\theta}\\theta_i^{\\alpha + X_i - 1}(1-\\theta_i)^{\\beta + N_i - X_i - 1} \\quad \\text{unnormalized Beta density}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2aeeff",
   "metadata": {},
   "source": [
    "### Hence $\\theta_i|X_i \\sim {\\rm Beta}(\\alpha + X_i, \\beta + N_i - X_i)$\n",
    "\n",
    "The fact that the posterior probability comes from the same distribution family as the prior is known as *conjugacy*. It is a very useful property because it allows us to compute the posteriors in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93581087",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1b) Conceptual\n",
    "When specifying a Bayesian model, we use our domain knowledge to establish certain distributions, and then we use computation to find other ones. Which of the following do we establish using our domain knowledge? Pick all that apply.\n",
    "\n",
    "(a) Prior\n",
    "\n",
    "(b) Likelihood\n",
    "\n",
    "(c) Marginal distribution of the data\n",
    "\n",
    "(d) Posterior "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e2e39",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb1f87",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 1c) Examine the prior distribution\n",
    "\n",
    "Before we jump into our COVID meta-analysis, let's start by gaining some intuition about our prior, the Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(alpha_value, beta_value):\n",
    "    x = np.arange(0, 1.01, 0.01)\n",
    "    y = beta.pdf(x, alpha_value, beta_value)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(r'$\\theta_i$')\n",
    "    plt.ylabel(r'$p(\\theta_i)$')\n",
    "    plt.title(r'Beta distribution with parameters $\\alpha$ and $\\beta$')\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278040ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_beta, alpha_value=(1, 20, 0.5), beta_value=(1,20, 0.5))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aee90f",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1c (i)\n",
    "Fix `alpha_value = 5`, and experiment with different values of `beta_value`. As we increase `beta_value`, what happens to the mode of the distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39f8a4",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640746a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1c (ii) \n",
    "Fix `beta_value = 5`, and experiment with different values of `alpha_value`. As we increase `alpha_value`, what happens to the mode of the distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e952a",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d1ad9",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1c (iii) \n",
    "Set `alpha_value = beta_value = 1`, increase their value such that `alpha_value=beta_value`. What happens to the *variance* of the distribution? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a3099",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34d0c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 1d) Compute Posterior Mean Estimates for SAR\n",
    "As we saw in [Discussion 3 Question 3](https://drive.google.com/file/d/1g0EJRD5a6RlJ2D_qVKcA3BRObzOzvBc6/view), the **posterior mean** minimizes the **Bayesian Posterior Risk** for the **Squared Error Loss**. In light of this fact, we'll use the mean of the posterior distribution to estimate the Secondary Attack Rate (SAR).\n",
    "\n",
    "**In the cell below, write a function that computes the posterior mean corresponding to $\\theta_i|X_i$.**\n",
    "\n",
    "*Hint: If you need to look up facts about certain well-known distributions, you can always (a) go to textbooks from classes you've taken before, (b) look on Wikipedia, or (c) do a simple web search.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2194fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def posterior_mean_estimate(N_value, X_value, alpha_value, beta_value):\n",
    "    \"\"\"\n",
    "    Computes the posterior E[theta_i|X_i] when we consider a prior theta_i ~ Beta(alpha, beta)\n",
    "    \n",
    "    Inputs: \n",
    "        N_value : int, total number of susceptible individuals\n",
    "        X_value : int, number of individuals that became infected\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "    \"\"\"\n",
    "    posterior_mean = ...\n",
    "    return posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2144386",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafaa0cc",
   "metadata": {},
   "source": [
    "## 1e) Examine the posterior mean estimate\n",
    "\n",
    "Let's assume that from domain knowledge, we think that the probability of infection (SAR) is close to $\\frac{1}{3}$. We pick a prior distribution for $\\theta_i$ that has mean $\\frac{1}{3}$. Any distribution of the form $\\theta_i \\sim {\\rm Beta}(k, 2k)$ has this property. The value of $k$ determines the 'strength' of the prior. Low values of $k$  correspond to 'flatter' priors, while larger values of $k$ correspond to 'peakier' priors. Play with the sliders in `q1c` to convince yourself.\n",
    "\n",
    "**Examine the plotting function below and answer the qualitative questions in the next cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b38d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify: Examine the code\n",
    "def plot_thetas(k):\n",
    "    \n",
    "    study_df[\"bayesian_theta\"] = study_df.apply(\n",
    "        lambda row: posterior_mean_estimate(row['N'], row['X'], k, 2*k), \n",
    "        axis=1\n",
    "    )\n",
    "    study_df[\"trivial_theta\"] = study_df.apply(\n",
    "        lambda row: trivial_theta_estimate(row['N'], row['X']), \n",
    "        axis=1\n",
    "    )\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    graph = sns.scatterplot(\n",
    "        x=\"trivial_theta\", y=\"bayesian_theta\", \n",
    "        data=study_df, size=\"N\", sizes=(50, 300), alpha=.8\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x='trivial_theta', y='trivial_theta', \n",
    "        data= study_df, ls=\"--\", color='black', lw=1\n",
    "    )\n",
    "    plt.ylim(0.16, 0.52)\n",
    "    graph.axhline(\n",
    "        1/3, color='black', \n",
    "        label = \"$\\frac{1}{3}$ Prior Expectation\"\n",
    "    )\n",
    "    plt.xlabel('Trivial Estimate')\n",
    "    plt.ylabel('Posterior Mean Estimate')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    x = np.arange(0,1.01,0.01)\n",
    "    y = beta.pdf(x, k, 2*k)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(r'$\\theta_i$')\n",
    "    plt.ylabel(r'$p(\\theta_i)$')\n",
    "    plt.title(rf'Prior: $Beta(\\alpha={k}, \\beta={2*k})$')\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876718b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_thetas, k=(0, 50, 2))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541cbcf",
   "metadata": {},
   "source": [
    "In the plot above the solid horizontal line represents the prior mean estimate $\\mathbb{E}[\\theta_i] = \\frac{k}{k+2k} = 1/3$. The dashed diagonal line marks $x=y$. Each data-point corresponds to a study, the size of the marker denotes the number of susceptible individuals in each study. Such that larger markers correspond to larger studies.\n",
    "\n",
    "**Based on your observations of the interactive plot above, answer the following questions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5fdb0",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1e (i) \n",
    "Start by setting $k=0$, and then steadily increase the value of $k$. Record your observations using **2-3 sentences** in the space below. Be sure to address the following questions:\n",
    "1. At $k=0$, where do the data points lie relative to the horizontal and diagonal lines? Why?\n",
    "2. As we increase the value of $k$, do the points move towards or away from the horizontal line? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028391f9",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd6b50",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1e (ii) \n",
    "\n",
    "As you increase $k$, some points move faster than others. Which points move faster, the larger or smaller data points? Explain why this is the case in **1-2 sentences**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3c82f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3af440",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1e (iii) \n",
    "Imagine that we let $k\\to \\infty$. How do you think the two graphs above will look in the limit $k\\to \\infty$? Limit your response to **1-2 sentences**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c6e58",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7a34f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1e (iv) \n",
    "\n",
    "Fill in the blank in this sentence with either \"small\" or \"large\", and explain your answer in **1-2 sentences**: \n",
    "\n",
    "*If we're very sure that the true SAR is close to $\\frac{1}{3}$, we should choose a _______ value of $k$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328a173",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6add40",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 2: Computational Approximate Inference\n",
    "\n",
    "In the previous question we looked at a Beta-Binomial Graphical model. We took advantage of the conjugacy properties of the model and were able to compute closed form solutions for the posterior mean estimates.\n",
    "\n",
    "However, as we introduce more complexity to the model, the **conjugacy property quickly breaks and we have to resort to approximate inference**. In this class, we'll focus primarily on *sampling* for approximate inference: this will be the topic of the next few lectures and next week's labs. In sampling-based approaches, **we don't even try to get the exact posterior: instead, we generate a bunch of samples from it, and use those to approximate the distribution.**\n",
    "\n",
    "In this question you will get a taste for probabilistic programming using `PyMC`. Spend some time perusing the [documentation](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html), but don't worry if there are parts that don't make sense yet. The Quickstart guide is a useful starting point. \n",
    "\n",
    "We'll be using PyMC to run an algorithm called Markov Chain Monte Carlo (MCMC), which you'll learn about this week and next. We'll start by using the same model from `Question 1`, and compare the results from MCMC with the exact solutions we calculated above. Then, we'll add an extra parameter to the model and make things more complex: even though we can no longer compute our posterior in closed form, MCMC will still generate samples that we can use to estimate each $\\theta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model so that one-time initialization\n",
    "# happens while you're reading over the code in the next cell.\n",
    "\n",
    "# Note: this and the following cells may take a while to run\n",
    "\n",
    "# You can ignore the output of this cell.\n",
    "\n",
    "with pm.Model() as model:\n",
    "    dummy = pm.Beta('dummy', alpha=1, beta=1)\n",
    "    pm.sample(1, return_inferencedata=False, progressbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09860c",
   "metadata": {},
   "source": [
    "## 2a) Building and sampling from a PyMC model\n",
    "\n",
    "First, let's get acquainted with the structure of PyMC, and show that methods of approximate inference are (in many cases) just as good as having access to the exact distribution. Just like in `Question 1`, we're still trying to estimate the SAR using the posterior mean; only this time, we'll be using PyMC to get an approximate answer!\n",
    "\n",
    "#### So, how does PyMC work? What does it even do?\n",
    "\n",
    "In essence, PyMC is just a number generator with extra steps. Given a *likelihood* and a *prior*, it simulates thousands of samples to create an empirical distribution that approximates the true posterior distribution. With those samples, we're able to calculate values of interest (such as the posterior mean, MAP, etc.), and create estimates of $\\theta$.\n",
    "\n",
    "#### How do we tell PyMC what our model looks like?\n",
    "\n",
    "As in any Bayesian parameter estimation problem, in order to find the posterior distribution, we must supply a **likelihood** and **prior**. To do this, we can instantiate [`Distribution`s](https://www.pymc.io/projects/docs/en/v3/Probability_Distributions.html) and their associated parameters to represent the random variables in our problem (you can take a look [here](https://www.pymc.io/projects/docs/en/stable/api/distributions.html) for a list of supported distributions).\n",
    "\n",
    "Once defined, these distribution objects are like any random variable in statistics; you can add, subtract, multiply, divide, and use mathematic operators on them (though sometimes you may need to use PyMC's own math operators). They can also function as either likelihoods or priors, depending on the parameters you feed them:\n",
    "1. If a `Distribution` object's parameters **do not** depend on other `Distribution` objects, these represent **priors**.\n",
    "2. If a `Distribution` object's parameters **do** depend on other `Distribution` objects, these represent **likelihoods**.\n",
    "\n",
    "#### What about our data?\n",
    "\n",
    "Now that we've specified a likelihood and prior, we have to consider our *data*; remember, the posterior describes the distribution of $\\theta$ given the data we observed! To do this, the `Distribution` class has a nifty little argument that we can pass in called `observed`. Using the `observed` argument, we can pass in our observed data to the random variable/`Distribution` object that corresponds to our observations, and let PyMC handle the rest!\n",
    "\n",
    "#### Now, try it out for yourself!\n",
    "\n",
    "Take a look at the cell below, and **define the theta and X nodes to approximate the posterior distribution.** The PyMC website has documentation for defining variables according to the Beta and Binomial distribution. If you have no idea how to proceed, the PyMC Quickstart guide is a great place to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9988aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def approximate_inference_MCMC(\n",
    "    alpha_value, beta_value, study_df = study_df\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and generates samples from a PyMC model of\n",
    "    the posterior distribution that corresponds to the\n",
    "    graphical model in Q.1, using Markov Chain Monte Carlo (MCMC)\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of\n",
    "        the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "        model is a PyMC model object, which represents the graphical model\n",
    "        trace is a PyMC trace object, which represents 2000 samples\n",
    "            of everything from the posterior\n",
    "    \"\"\"\n",
    "    # Defines the graphical model\n",
    "    with pm.Model() as model:\n",
    "        # The prior for theta is a Beta distribution with parameters\n",
    "        # alpha and beta, and there's one for each study.\n",
    "        # Make sure to name this parameter 'theta' so so this lab can reference it later. \n",
    "        theta = ...\n",
    "        \n",
    "        # The likelihood for X is binomial, with parameter p=theta,\n",
    "        # observed counts in study_df['X'], and observed N similarly\n",
    "        X = ...\n",
    "        \n",
    "        # Generate samples from the posterior distribution using : run 4\n",
    "        # Markov chains of sampling in parallel, generating 500 samples\n",
    "        # each.\n",
    "        trace = pm.sample(500, chains=4, tune=1000, target_accept=0.95, return_inferencedata=False, progressbar=False)\n",
    "    \n",
    "    return (model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cbc79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82cda39",
   "metadata": {},
   "source": [
    "Now, when we call the `approximate_inference_MCMC` function, we run the sampler and pass in fixed values of the hyperparameters alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6921b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run approximate inference\n",
    "model, trace = approximate_inference_MCMC(10, 20)\n",
    "\n",
    "# Get posterior samples of theta\n",
    "thetas = trace['theta']\n",
    "thetas\n",
    "print(thetas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ef093",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Note that the shape of ``thetas`` is (N x M).  What are N and M, and what does each mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0d68d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee966d3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2b) Using the output of PyMC\n",
    "\n",
    "Now that we've run our sampler, we now have access to the posterior distributions of *all* the random variables we defined in PyMC. Using these empirical distributions, we can now calculate the posterior means for each $\\theta_i$. But before we do that, let's visualize the samples we got back.\n",
    "\n",
    "Generate a histogram of all 2,000 posterior samples for $\\theta_2$ (the SAR for Study 2). Use the `sns.histplot` function with `stat='density'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4690337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Create histogram of posterior samples\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f790d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "How do the samples compare to the two different estimates you saw in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef864a0",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1695bf",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2c) Compute Posterior Mean Estimates from Samples\n",
    "\n",
    "Fill in the function that computes posterior mean estimates for each $\\theta_i$ for different parameters $\\alpha, \\beta$ of the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae8ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def empirical_posterior_mean_estimates(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\" \n",
    "    Computes posterior mean estimates of theta_i by performing approximate inference\n",
    "    and then sampling from the posterior distribution:\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Output:\n",
    "        posterior_estimates : (num_studies,) 1-D array of the same length as the \n",
    "            number of studies. posterior_estimates[i] contains the \n",
    "            mean estimate for theta_i based on running MCMC\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    posterior_estimates = ...\n",
    "    return posterior_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73196d0a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1b6d2",
   "metadata": {},
   "source": [
    "## 2d) Comparing Results from Question 1 and Question 2\n",
    "\n",
    "Now that we've utilized conjugacy to get the theoretical distribution of the exact posterior (`Question 1`) and applied Markov Chain Monte Carlo to get an approximate empirical distribution (`Question 2`), let's compare the two to see how good our approximation was!\n",
    "\n",
    "Below, we've provided code to make a 4x3 plot such that each subplot corresponds to a study. \n",
    "\n",
    "Each subplot contains 2 curves and a frequency histogram:\n",
    "- The PDF of the prior distribution of $\\theta_i$\n",
    "- The PDF of the true posterior distribution $\\theta_i|X_i$ computed in closed form, as in Q.1\n",
    "- The histogram of posterior samples of $\\theta_i|X_i$ computed in Q.2\n",
    "\n",
    "Take a look through the subplots, and compare the fit between our empirical and theoretical distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_densities(alpha_value, beta_value, study_df = study_df): \n",
    "    \"\"\"\n",
    "    Plots for each study the prior distribution, true posterior,\n",
    "    and histogram of posterior samples using MCMC\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Outputs:\n",
    "        fig : Figure with 12 subplots\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(4, 3)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    \n",
    "    theta = np.arange(0, 1.01, 0.01)\n",
    "    prior = beta.pdf(theta, alpha_value, beta_value)\n",
    "    \n",
    "    model, trace = approximate_inference_MCMC(alpha_value, beta_value, study_df) \n",
    "    samples = trace['theta'] \n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            idx = 3*i+ j\n",
    "            X_i = study_df.loc[idx, 'X']\n",
    "            N_i = study_df.loc[idx, 'N']\n",
    "            study_name = f'Study {idx}'\n",
    "            true_posterior = beta.pdf(theta, alpha_value+X_i, beta_value+N_i-X_i) \n",
    "            \n",
    "            ax = axs[i, j]\n",
    "            ax.plot(theta, prior, label = 'Prior')\n",
    "            ax.plot(theta, true_posterior, label = \"Theoretical Posterior\")\n",
    "            ax.hist(samples[:,idx], label = \"Empirical Posterior\", density=True, alpha = 0.7)\n",
    "            ax.set_title(study_name)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6682ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a weak prior\n",
    "fig1 = plot_densities(2, 4, study_df = study_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a strong prior\n",
    "fig2 = plot_densities(20, 40, study_df = study_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a0e40",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2d (i) \n",
    "\n",
    "Compare the curve of the theoretical distribution with the histogram of samples from the empirical posterior. Are they similar or different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848866a2",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512b5a3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2d (ii)\n",
    "\n",
    "Compare the two figures corresponding to 'weak' prior $\\theta_i \\sim Beta(2,4)$ and 'strong' prior  $\\theta_i \\sim Beta(20,40)$. How are they different? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22b43c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11a10b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2e) Approximate Inference for a More Complex Model\n",
    "\n",
    "The previous 2 parts served as a sanity check that the approximate inference techniques used by PyMC can approximate the theoretical posterior. The usefulness of such packages becomes apparent when we are dealing with more complex models that don't have conjugacy properties.\n",
    "\n",
    "Consider the following graphical model:\n",
    "\n",
    "![](model_asymptomatic.png)\n",
    "\n",
    "Recent studies have shown that a large fraction of COVID cases do not show symptoms, but all of the studies considered here tested only symptomatic cases. This means that the probability of testing positive (which what we observe) isn't the same as the SAR $\\theta_i$! \n",
    "\n",
    "The estimates of the asymptomatic rate fall in the range $[0.18, 0.43]$. We assume a prior $A\\sim Uniform(0.18, 0.43)$. This means that the probability that a person in a study tests positive is really $\\theta_i*(1-A)$. Hence:\n",
    "\n",
    "$$X_i|\\theta_i, A \\sim Binomial(N_i, \\theta_i\\cdot (1 - A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff30995",
   "metadata": {},
   "source": [
    "#### Complete the `approximate_inference_asympotmatic_MCMC` function to add dependence on the asymptomatic rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b6042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def approximate_inference_asympotmatic_MCMC(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\"\n",
    "    Creates and fits a PyMC model corresponding to the graphical model above\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "    \"\"\"\n",
    "    with pm.Model() as model:\n",
    "        theta = ...\n",
    "        A = ...\n",
    "        X = ...\n",
    "        \n",
    "        trace = pm.sample(500, tune=1000, target_accept=0.95, return_inferencedata=False, progressbar=False)\n",
    "    return (model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aabe91",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2e_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c412f",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Notice that the `trace` now contains samples for both `theta` and `A`!\n",
    "\n",
    "Plot a histogram of the posterior estimates for $A$ if $\\alpha=5$ and $\\beta=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477a147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, trace = approximate_inference_asympotmatic_MCMC(5, 10)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36fb67",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Assuming the model we defined is correct, what can you conclude about the asymptomatic rate $A$ based on the studies and the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb9247",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18273e3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Well done!\n",
    "You've reached the end of the lab! Make sure you double check your work and make sure that you've answered all the written portions of the lab.\n",
    "\n",
    "Before you submit to Gradescope, make sure you pass all the autograded portions of this lab. **Run the cell below to generate a PDF of your lab submission**, and **run the last cell to generate a zip file of your lab submission.** Do **not** create your lab PDF by exporting your notebook to a PDF.\n",
    "\n",
    "To submit your lab to Gradescope, submit the PDF to Lab 4 Written and the zip file to Lab 4 Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab04.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab04.pdf')):\n",
    "    img = mpimg.imread('baby_rock_hyrax.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    display(HTML(\"Download your PDF <a href='lab04.pdf' download>here</a>.\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e570dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b597b1e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e62164",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4923f38",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n_test_array = [10, 100, 1000]\n>>> x_test_array = [10, 34, 852]\n>>> res_array = [trivial_theta_estimate(n, x_test_array[i]) for i,n in enumerate(n_test_array)]\n>>> assert np.allclose(res_array, [1.0, 0.34, 0.852])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N_test = 100\n>>> X_test = 20\n>>> alpha_test_array = [1, 10, 100]\n>>> beta_test_array = [1, 10, 100]\n>>> inputs = list(itertools.product(alpha_test_array, beta_test_array))\n>>> outputs = [posterior_mean_estimate(N_test, X_test, *inp) for inp in inputs]\n>>> assert np.allclose(outputs, [0.20588235294117646,\n...  0.1891891891891892,\n...  0.1044776119402985,\n...  0.2702702702702703,\n...  0.25,\n...  0.14285714285714285,\n...  0.5970149253731343,\n...  0.5714285714285714,\n...  0.4])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a_i": {
     "name": "q2a_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> logger.setLevel(logging.ERROR)\n>>> model, trace = approximate_inference_MCMC(10, 20)\n>>> thetas = trace['theta']\n>>> logger.setLevel(logging.INFO)\n>>> assert thetas.shape == (2000,12)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> logger.setLevel(logging.ERROR)\n>>> nreps = 2\n>>> \n>>> answers = [[0.3],\n...              [0.26],\n...              [0.34],\n...              [0.31],\n...              [0.29],\n...              [0.33],\n...              [0.32, 0.33],\n...              [0.28],\n...              [0.23],\n...              [0.37],\n...              [0.43],\n...              [0.35]]\n>>> \n>>> checks = []\n>>> for _ in range(nreps):\n...     posterior_estimates_test = empirical_posterior_mean_estimates(10,25)\n...     checks.append([np.round(posterior_estimates_test[i],2) in answers[i] for i in range(12)])\n>>> logger.setLevel(logging.INFO)\n>>> assert np.any(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2e_i": {
     "name": "q2e_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nreps = 2\n>>> logger.setLevel(logging.ERROR)\n>>> answers = [0.38, 0.3, 0.44, 0.38, 0.36, 0.42, 0.42, 0.36, 0.28, 0.48, 0.56, 0.44]\n>>> checks = []\n>>> for _ in range(nreps):\n...     model_test, trace_test = approximate_inference_asympotmatic_MCMC(5, 10)\n...     post_samples_test = trace_test['theta']\n...     estimates = np.mean(post_samples_test, axis = 0)\n...     rounded_estimates = np.round(estimates / 2, 2) * 2\n...     checks.append(np.all([rounded_estimates[i] == answers[i] for i in range(len(rounded_estimates))]))\n>>> logger.setLevel(logging.INFO)\n>>> assert np.any(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
