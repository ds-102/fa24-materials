{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e7de3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9ce54",
   "metadata": {},
   "source": [
    "# Lab 6: GLMs and the Bootstrap\n",
    "Welcome to the Data 102 Lab 6. In this lab, we are going to look at a variety of topics including model selection, confidence intervals and credible intervals, the bootstrap, and hypothesis testing. The lab assignment may seem long, but there are only a few simple lines of code to fill in in this lab assignment. There is a lot of text in this assignment. Please make sure you read carefully.\n",
    "\n",
    "#### The code and responses you need to write are represented by `...`. There is additional documentation for each part as you go along.\n",
    "\n",
    "##### Please read carefully the introduction and the instructions for each problem.\n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "You can submit the lab in pairs (groups of two, no more than two). **If you choose to work in a pair, please make sure to add your group member on Gradescope for both Lab 06 written submission and the Lab 06 code submission.**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually and do not share your code with anyone other than your partner**. If you do discuss the assignments with people other than your partner please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2eef84",
   "metadata": {},
   "source": [
    "**Collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f7ee8",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Again, since this lab involves sampling, **tests may take awhile to run. Submit as early as possible, as last minute submissions may overwhelm datahub, preventing yourself and others from submitting on-time.** \n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Friday, Oct 11th, 2024 at 05:00 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interactive\n",
    "import itertools\n",
    "import hashlib\n",
    "from scipy.stats import poisson, norm, gamma\n",
    "#!pip install pymc3\n",
    "import statsmodels.api as sm\n",
    "  \n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from pymc import *\n",
    "import pymc as pm\n",
    "import bambi as bmb\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7842f",
   "metadata": {},
   "source": [
    "## Review: GLMs [Adapted from Lab 05]\n",
    "\n",
    "**If you completed Q3 in Lab 5, you can copy and paste your answers into the Lab 6 notebook (Q0) to avoid duplicating effort.**\n",
    "\n",
    "Now, we will pivot to discussing frequentist and Bayesian approaches to generalized linear models. This question will be easier after Tuesday's lecture!\n",
    "\n",
    "## Question 0: Atlantic Hurricane Season\n",
    "\n",
    "With 30 named storms, the 2020 Atlantic hurricane season was the most active on record. Climate scientists argue that the culprit is human induced global warming. There is a an evergrowing body of research linking increased average temperatures and rising sea levels to more frequent, more intense and more destructive storms. \n",
    "\n",
    "In this lab we will investigate the number of named storms recorded since 1880, and we will argue that there is a statistically significant relationship between rising Sea Surface Temperature (SST) and the frequency of named storms.\n",
    "\n",
    "For this lab we extracted the number of tropical storms from the [HURDAT Database](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2019-052520.txt). We also extracted data on Sea Surface Temperatures from the [National Center for Atmosferic Research](https://climatedataguide.ucar.edu/climate-data/global-surface-temperature-data-gistemp-nasa-goddard-institute-space-studies-giss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff20a0f",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run the code to load the data\n",
    "data_source = \"hurricane_data.csv\"\n",
    "df = pd.read_csv(data_source)\n",
    "df = df[[\"Year\", \"Num_Storms\", \"Temp_Anomaly\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87112d3c",
   "metadata": {},
   "source": [
    "### Model Specifications\n",
    "\n",
    "The `Num_Storms` column contains the number of named storms recorded each year between 1880 and 2019. The `Temp_Anomaly` column contains the deviation in yearly SST from the mean of 1951-1980.\n",
    "\n",
    "In this question, to show that there is a statistically significant relationship between rising Sea Surface Temperature (SST) and the frequency of named storms, we will model the number of named storms in Year $i$ using **Poisson Regression**:\n",
    "\n",
    "$$\\lambda_i = e^{q_0 + q_1 X_i}$$ \n",
    "\n",
    "$$C_i \\sim \\text{Poisson}(\\lambda_i),$$\n",
    "\n",
    "where $X_i$ is the SST deviation in Year $i$, and $C_i$ is the number of named storms in year $i$.\n",
    "\n",
    "This isn't something that we can easily solve from scratch, so we have to use software packages. In this question, we'll explore the two approaches to GLMs that we covered in class: \n",
    "\n",
    "1. **Q0(b): Frequentist Regression** using [`statsmodels.api`](https://www.statsmodels.org/stable/glm.html) \n",
    "2. **Q0(c) Bayesian Regression** via sampling using [`PyMC`](https://www.pymc.io/welcome.html) and [`Bambi`](https://bambinos.github.io/bambi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087db61",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 0(a) Understanding Check\n",
    "\n",
    "The model we described above is a GLM. What is \"Linear\" about this GLM model? What's the inverse link function? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a40075",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef9146",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 0(b) Frequentist Regression\n",
    "\n",
    "Let's start by considering the problem from a frequentist lens. To do this, we'll use the `statsmodels.api`, which allows us to create a model in just a few lines of code.\n",
    "\n",
    "After fitting our model, we can call the `.summary()` method, and get a breakdown of our model and some details on how well it fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(df[\"Num_Storms\"], exog = sm.add_constant(df[\"Temp_Anomaly\"]), \n",
    "                  family=sm.families.Poisson())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52eec81",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 0(b)(i) Understanding the table\n",
    "\n",
    "What variable does `Temp_Anomaly`'s `coef` in the table correspond to in our model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2691c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ee124",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 0(b)(ii) Inspecting the results of fitting `freq_model`. \n",
    "\n",
    "Does the model suggest that increased SST relate to more storms? Is the influnce of SST on number of storms statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1d0dc",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee85bc",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 0(c) Bayesian Regression via PyMC\n",
    "\n",
    "Now that we've done Poisson regression the frequentist way with the `statsmodels` package, let's try implementing it the Bayesian way! In this lab we'll explore two ways of doing this:\n",
    "1. Building the model from scratch in `PyMC`\n",
    "2. Using `Bambi`, a wrapper on `PyMC` that simplifies model construction\n",
    "\n",
    "To start, let's build our model from scratch in `PyMC`. Unlike the `statsmodels` package, `PyMC` requires us to specify our model piece by piece. That means that, as with any Bayesian parameter estimation task, we have to distill our problem setup into a likelihood and prior before we can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3710568",
   "metadata": {},
   "source": [
    "### 0(c)(i) Unpacking $C_i \\sim \\text{Poisson}(\\lambda_i)$\n",
    "\n",
    "Recall the problem setup:\n",
    "\n",
    "Our model involves the relationship between rising Sea Surface Temperature (SST) and the frequency of named storms, where:\n",
    "\n",
    "$$\\lambda_i = e^{q_0 + q_1 X_i}$$ \n",
    "\n",
    "$$C_i \\sim \\text{Poisson}(\\lambda_i),$$\n",
    "\n",
    "where $X_i$ is the SST deviation in Year $i$, and $C_i$ is the number of named storms in year $i$.\n",
    "\n",
    "**In the cell below, Choose the option that best fills in the blank in the following statement:**\n",
    "\n",
    "$C_i \\sim \\text{Poisson}(\\lambda_i)$ represents our _____:\n",
    "\n",
    "**A.** Prior\n",
    "\n",
    "**B.** Likelihood\n",
    "\n",
    "**C.** Posterior\n",
    "\n",
    "Your answer should be a string, either `\"A\"`, `\"B\"`, or `\"C\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q0c_i = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d6cd0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0c_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e50101",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 0(c)(ii) Picking our prior(s)\n",
    "\n",
    "Now, let's examine the parameter $\\lambda_i = e^{q_0 + q_1 X_i}$. As discussed in lecture, $\\lambda_i$ is comprised of a linear function of our observations ($q^TX$) and an inverse-link function ($e$). Given the setup of our problem, answer the following questions using 1 sentence each:\n",
    "1. How many prior distributions do we need to define?\n",
    "2. What parameters will we define these prior distributions for?\n",
    "3. Since we don't have any prior information about these random variables, what distribution (i.e Normal, Beta, etc.) should we pick for their priors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ed0a1",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d89f08",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 0(c)(iii) Defining our PyMC Model\n",
    "Given your answers to `q2c_i` and `q2c_ii`, you're now ready to make your PyMC model! Fill out the code cell below to build your model and sample from the posterior.\n",
    "\n",
    "**Note:** To pass the test, \n",
    "1. Do not remove the variables currently present in the model and/or add your own additional variables. Just fill in any ellipsis; the variables defined here are more than enough for you to solve the question!\n",
    "2. Make sure the name parameter you pass to each `Distribution` object matches the variable name it's assigned to. Your answers should follow the following format: \n",
    "\n",
    "```\n",
    "with pm.Model() as model:\n",
    "\n",
    "    q0 = pm.Some_Distribution('q0', ...)\n",
    "    q1 = pm.Some_Distribution('q1', ...)`\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Hint 1**: Remember that random variables can be added, subtracted, multiplied, etc. just like normal numbers!\n",
    "\n",
    "**Hint 2**: Not all variables defined in a model context necessarily have to be defined using `pm.Some_Distribution(...)`. In particular, we do not need `pm.Deterministic` when defining `lam`, since we're not interested in its posterior samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140c95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    q0 = ...\n",
    "    q1 = ...\n",
    "\n",
    "    lam = ...\n",
    "    \n",
    "    Y_obs = ...\n",
    "\n",
    "    # DO NOT CHANGE THE SAMPLING ROUTINE\n",
    "    trace = pm.sample(2000, chains = 4, random_seed = 0, return_inferencedata = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c7ee4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0c_iii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04885d3",
   "metadata": {},
   "source": [
    "Now that we've ran PyMC, we can visualize the posterior distributions of $q_0$ and $q_1$ and find our estimates $\\hat{q}_0$ and $\\hat{q}_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e40fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, round_to=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e8fd3",
   "metadata": {},
   "source": [
    "## 0(d) Bayesian Regression via Bambi\n",
    "\n",
    "Whew! `q0(c)` took *a lot* of code just to build a simple Poisson regression. In practice, building models from scratch like this is usually unnecessary unless we're looking for a custom solution. Instead, we can rely on packages like `Bambi` that *wrap* the functionality of `PyMC` with a simpler interface purely designed for GLMs.\n",
    "\n",
    "Using [`Bambi` documentation](https://bambinos.github.io/bambi/) as a guide, fill out the code cell below to fit a Bayesian Poisson Regression model on our data. \n",
    "\n",
    "**Note 1**: To pass the autograder, make sure you pass your model the `random_seed = 0` when you fit it!\n",
    "\n",
    "**Note 2**: Notice that the `Bambi` package doesn't need us to specify a prior: when we don't specify a prior for our model parameters, Bambi automatically supplies a weakly informative one based on your data by default. For this question, we are okay with this behavior: **to pass the test, do not specify a prior on your parameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663458c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model = ...\n",
    "my_model_samples = ...\n",
    "\n",
    "az.plot_posterior(my_model_samples, round_to=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19592c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81232a",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 0(e) Understanding the plots\n",
    "What the are x-axis and y-axis in each of the plots in 3d)? Your answer should be in terms of the parameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c776bfc",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5ec03",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 0(f) Comparison\n",
    "\n",
    "Compare the results of `freq_model` in 0(b) with the plot in 0(d). Are the estimates of Frequentist and Bayesian Regression close to each other? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db94f70",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a0af",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part I: Model Checking and Uncertainty Quantification\n",
    "## Question 1: Atlantic Hurricane Season\n",
    "\n",
    "In Lab 5, we explored hurricane data from the 2020 Atlantic hurrican season. Just like in the last lab, we're investigating the number of named storms recorded since 1880 and examining the relationship between rising Sea Surface Temperature (SST) and the frequency of named storms.\n",
    "\n",
    "For this lab we extracted the number of tropical storms from the [HURDAT Database](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2019-052520.txt). We also extracted data on Sea Surface Temperatures from the [National Center for Atmosferic Research](https://climatedataguide.ucar.edu/climate-data/global-surface-temperature-data-gistemp-nasa-goddard-institute-space-studies-giss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0e4f9",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d723a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run the code to load the data\n",
    "data_source = \"hurricane_data.csv\"\n",
    "df = pd.read_csv(data_source)\n",
    "df = df[[\"Year\", \"Num_Storms\", \"Temp_Anomaly\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a275e6",
   "metadata": {},
   "source": [
    "### Model Specifications\n",
    "\n",
    "The `Num_Storms` column contains the number of named storms recorded each year between 1880 and 2019. The `Temp_Anomaly` column contains the deviation in yearly SST from the mean of 1951-1980.\n",
    "\n",
    "As in Lab 5, we will model the number of named storms in Year $i$ using **Poisson Regression**:\n",
    "\n",
    "$$\\lambda_i = e^{q_0 + q_1 X_i}$$ \n",
    "\n",
    "$$C_i \\sim \\text{Poisson}(\\lambda_i),$$\n",
    "\n",
    "where $X_i$ is the SST deviation in Year $i$, and $C_i$ is the number of named storms in year $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5500d0d",
   "metadata": {},
   "source": [
    "## 1(a) Posterior Predictive Checks\n",
    "\n",
    "In order to validate our Bayesian model we perform Posterior Predictive Checks (PPC). After performing Bayesian Regression we have access to a generating distribution for counts $C'_i|X_i$. The crux of PPC is to sample such counts and to compare them to the original historical data.\n",
    "\n",
    "The code below computes PPC samples and plots their distribution. Note that here, $C$ has been renamed $y$, and the band labeled \"Posterior predictive $y$\" is a collection of curves, each of which is the density plot of $y$ for a given draw of the coefficient vector $\\beta$ from the posterior. \n",
    "\n",
    "Specifically to generate a red line we follow these steps: First, we take one of our posterior samples of $q_0$ and $q_1$. Then we draw samples from the distribution $p(Y_i| q_0,q_1, X_i)$ for each data point $X_i$. Then we plot the density of the resulting $Y$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify\n",
    "with pm.Model() as poisson_model:\n",
    "    q0 = pm.Flat('q0') # SOLUTION\n",
    "    q1 = pm.Flat('q1') # SOLUTION\n",
    "\n",
    "    lam = np.exp(q0 + q1 * df['Temp_Anomaly']) # SOLUTION\n",
    "    \n",
    "    y = pm.Poisson('y', mu=lam, observed=df['Num_Storms']) # SOLUTION\n",
    "\n",
    "    # DO NOT CHANGE THE SAMPLING ROUTINE\n",
    "    trace_poisson = pm.sample(1000, cores=2, target_accept=0.95, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a027fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Sample C'_i|X_i \n",
    "with poisson_model:\n",
    "    poisson_ppc = pm.sample_posterior_predictive(trace_poisson)\n",
    "    poisson_ppc.posterior_predictive['y'] = poisson_ppc.posterior_predictive['y'] + 0.0\n",
    "    poisson_ppc.observed_data['y'] = poisson_ppc.observed_data['y'] + 0.0\n",
    "    #poisson_ppc['y'] = poisson_ppc['y'] + 0.0\n",
    "    #ppc_poisson = poisson_ppc.posterior_predictive\n",
    "        \n",
    "# Plot PPC samples\n",
    "\n",
    "az.plot_ppc(poisson_ppc)\n",
    "plt.xlabel('y = Number of named storms')\n",
    "plt.title('Bayesian Poisson Regression with SST covariate')\n",
    "plt.axis([-2, 45, -0.01, 0.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_ppc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7854f",
   "metadata": {},
   "source": [
    "To check how important the SST (our data $X_i$) is to the results above, we can estimate a new Bayesian model which doesn't use the covariates $X_i's$, and instead uses a simple conjugate prior.\n",
    "\n",
    "This simple model is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here: Just inspect the code and make sure you understand what is happening\n",
    "with pm.Model() as simple_bayes_model:\n",
    "    mu = pm.Gamma('mu', alpha=10, beta=1)\n",
    "    counts = pm.Poisson('C', mu=mu, observed=df.Num_Storms)\n",
    "    simple_trace_poisson = pm.sample(1000, cores=2, target_accept=0.95, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Sample C'_i\n",
    "with simple_bayes_model:\n",
    "    simple_bayes_ppc = pm.sample_posterior_predictive(simple_trace_poisson)\n",
    "    simple_bayes_ppc.posterior_predictive['C'] = simple_bayes_ppc.posterior_predictive['C'] + 0.0\n",
    "    simple_bayes_ppc.observed_data['C'] = simple_bayes_ppc.observed_data['C'] + 0.0\n",
    "    #ppc_poisson = poisson_ppc.posterior_predictive\n",
    "        \n",
    "# Plot PPC samples\n",
    "\n",
    "az.plot_ppc(simple_bayes_ppc)\n",
    "plt.xlabel('C = Number of named storms')\n",
    "plt.title('Bayesian Poisson Rate Estimation without SST covariate')\n",
    "plt.axis([-2, 45, -0.01, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152c269",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1(a)(i) With and without SST\n",
    "\n",
    "Compare the two plots above. In your opinion, which model is a better fit for the observed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936d5ac",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c9a46",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part II: Bootstrap and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a46a04",
   "metadata": {},
   "source": [
    "And now for something completely different. In the following questions, we are going to experiment with bootstrap and hypothesis testing. You will learn about non-parametric density estimation and the critical role of smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af1c01",
   "metadata": {},
   "source": [
    "## Goal: testing for multimodality\n",
    "Suppose that $X_1, . . . , X_n$ are i.i.d. samples from a distribution with continuous density $p(x)$.\n",
    "One important property of the density $p(x)$ is the number of modes it has. Multimodality of\n",
    "the density indicates a heterogeneity in the data. In this lab, we will demonstrate how to perform a hypothesis test to determine whether a distribution is multimodal. We'll use the Bootstrap Method to perform this hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2f3bc",
   "metadata": {},
   "source": [
    "## Galaxy data\n",
    "\n",
    "We will be working with galaxy data. The dataset contains velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey\n",
    "of the Corona Borealis region. The distribution\n",
    "of galaxy velocities provides information about the structure of the far universeâ€”in\n",
    "particular, a multimodal distribution of velocities is seen as evidence for the existence\n",
    "of voids and superclusters (links to wikipedia pages on [voids](https://en.wikipedia.org/wiki/Void_(astronomy)) and [superclusters](https://en.wikipedia.org/wiki/Supercluster)).\n",
    "\n",
    "Let $X_1, . . . , X_{n}$ be the velocities of each galaxy, where $X_i$ is the velocity of the $i$th galaxy and we observe $n=82$ galaxies.\n",
    "\n",
    "We want to test whether or not the distribution that the $X_i$'s are drawn from is multimodal. Let the null and alternative hypotheses be defined as follows:\n",
    "\n",
    "$$H_0: m(p) = 1$$ \n",
    "$$H_A: m(p) > 1$$ \n",
    "\n",
    "where $p$ is the distribution of galaxy velocities, and $m(p)$ is the number of modes of a distribution $p$.\n",
    "\n",
    "The formal definition of a mode will be given in Question 3 below. For now, just think of a mode as a distinct \"hump\" of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2391d",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First, we'll load the data and see what the histogram looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_df = pd.read_csv('galaxies.csv', index_col=0, header=0, names=['velocity'])\n",
    "# Divide all entries by 1000 for ease of reading.\n",
    "galaxies_df['velocity'] = galaxies_df['velocity'] / 1000\n",
    "X_observed = np.array(galaxies_df['velocity'])\n",
    "galaxies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca67e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_observed, bins=15)\n",
    "plt.title(\"Histogram of galaxy velocities\")\n",
    "plt.xlabel(\"Velocity of galaxy, X (thousands of km/s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2605d",
   "metadata": {},
   "source": [
    "Note that from our quick glance at the histogram, it looks like there are approximately three modes. One around 10, one around 20, and one around 32. In the remainder of the lab, we'll test for multiple modes formally with a hypothesis test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42b17f",
   "metadata": {},
   "source": [
    "# Question 2. Estimating the density and test statistic\n",
    "\n",
    "In order to infer whether or not the $X_1,...,X_{n}$ were drawn from a multimodal distribution, we need to come up with a test statistic that reflects how suitable a unimodal distribution is for\n",
    "modeling this data. \n",
    "\n",
    "To do this, we first need to come up with a model for the density function itself. In this lab you will have the chance to learn about a non-parametric density estimatation technique called *kernel density estimation*. This was also covered briefly in Data 100. To revisit the concept, see [data 100 textbook](https://www.textbook.ds100.org/ch/11/viz_smoothing.html).\n",
    "\n",
    "#### Kernel Density Estimation\n",
    "Given a set of points $X_1, X_2, \\ldots, X_n \\sim p(x)$. The goal of Kernel Density Estimation is to estimate the density $p(x)$ via a function $\\hat{p}_h(x)$ of the form:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{p}_h(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \n",
    "\\end{align}\n",
    " \n",
    "\n",
    "The function $K$ is a non-increasing function that takes only non-negative values. These functions are known as kernel functions. They are often used to capture the influence of each data point $X_i$ on the density of an arbitrary point $x$. A common choice of kernel is the Gaussian kernel, which is what we will use in this lab: \n",
    "\n",
    "$$K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf720b1",
   "metadata": {},
   "source": [
    "In addition, the parameter $h > 0$ is a bandwidth parameter that captures how close data points $X_i$ must be to $x$ to influence its density: for larger values of $h$, more data points have an\n",
    "influence on the density at $x$, whereas for smaller values of $h$, only data points very close\n",
    "to $x$ influence it.\n",
    "\n",
    "It can be shown that the number of modes of $\\hat{p}_h(x)$ (a.k.a. $m(\\hat{p}_h(x))$) decreases monotonically as $h$ increases. Therefore, $h$ will be an important tool in our hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5fe0",
   "metadata": {},
   "source": [
    "## 2.a. Plot the density estimates $\\hat{p}_h(x)$\n",
    "\n",
    "Using the kernel function $K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, we will first plot $\\hat{p}_h(x)$ to get a sense of what these density estimates look like for different values of $h$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p}_h(x) &= \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "&= \\frac{1}{nh \\sqrt{2\\pi}} \\sum_{i=1}^n \\exp\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Using the final simplified form above, implement a function that calculates $\\hat{p}_h(x)$ at a given point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90c1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import pi, exp, sqrt\n",
    "def p_hat(x, h, X):\n",
    "    \"\"\"Calculates p_hat(x) at a single point x, where the bandwidth of the kernel function is h.\n",
    "    \n",
    "    Inputs: \n",
    "      x : float, point at which to evaluate the function p_hat.\n",
    "      h : float, bandwidth parameter in kernel function.\n",
    "      X : array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      y: float, the value of p_hat(x) at the given point x.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6086bbc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce281bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run this cell after you pass the validation tests above\n",
    "def plot_density_estimate(h):\n",
    "    x_values = np.arange(0, 45, 0.5)\n",
    "    y_values = [p_hat(x, h, X_observed) for x in x_values]\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    plt.hist(X_observed, bins=15, density=True, label=\"Histogram of observed values\")\n",
    "    plt.plot(x_values, y_values, label = \"Estimated kernel density\")\n",
    "    plt.title(\"Density $\\hat{p}_h(x)$\")\n",
    "    plt.ylabel(\"Density $\\hat{p}_h(x)$\")\n",
    "    plt.xlabel(\"Velocity, x\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactive plot: Do not modify\n",
    "interactive_plot = interactive(plot_density_estimate, h=(0.1, 4, 0.1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a706f",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.b. Questions:\n",
    "### (i) Start with a small value of $h=0.1$, then slide the value of $h$, what do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8335b",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872d881",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (ii) Does the density estimate $\\hat{p}_h(x)$ seem to contain more modes for higher values of $h$ or lower values of $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5636ddd",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1168d1a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (iii) For what values of $h$ (small or large), does $\\hat{p}_h(x)$ fit the current data more closely? Would this value generalize well to other unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d33f2",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb494a4",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 3. Count the modes of $\\hat{p}_h(x)$\n",
    "\n",
    "### There are no todo's in this question except for a simple fill-in-the-blank question at the end. Make sure you read through the questions and understand the solutions before you move on.\n",
    "\n",
    "Now we will write a function that counts the number of modes of a given density estimate $\\hat{p}_h(x)$. This is the $m(p)$ function mentioned above. \n",
    "\n",
    "To do this, we say that a density function $p$ has a mode everywhere the function $p(x)$ has an increase followed by a decrease. That is, $p(x)$ has an additional mode for each time the derivative of the function $p(x)$ transitions from non-negative to negative.\n",
    "\n",
    "Following the above definition, to count the number of modes in $\\hat{p}_h(x)$, first we will take the derivative, $$\\frac{d}{dx}\\hat{p}_h(x).$$\n",
    "\n",
    "Then, we will count the number of times that the derivative transitions from positive (or 0) to negative over a grid of $x$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4121d",
   "metadata": {},
   "source": [
    "## 3.a. Calculate the derivative $\\frac{d}{dx}\\hat{p}_h(x).$\n",
    "\n",
    "Using the kernel function $K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, we will now calculate the derivative $\\frac{d}{dx}\\hat{p}_h(x)$ by applying the chain rule.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}\\hat{p}_h(x) &= \\frac{d}{dx} \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{d}{dx} K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{1}{h} K'\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh^2} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\frac{-(x - X_i)}{h} \\exp\n",
    "\\left(-\\frac{\\left(\\frac{x - X_i}{h}\\right)^2}{2}\\right) \\\\\n",
    "&= \\frac{1}{nh^3 \\sqrt{2\\pi}} \\sum_{i=1}^n (X_i - x)\\exp\n",
    "\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Using the final simplified form of the derivative above, we will then implement a function that calculates $\\frac{d}{dx}\\hat{p}_h(x)$ at a given point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat_derivative(x, h, X):\n",
    "    \"\"\"Calculates the derivative d/dx p_hat(x) at a single point x.\n",
    "    \n",
    "    Inputs: \n",
    "      x : float, point at which to evaluate the derivative.\n",
    "      h : float, bandwidth parameter in p_hat.\n",
    "      X : array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      y_prime : float, the derivative d/dx phat_h(x) at the given point x.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    total = np.sum((X - x) * np.exp(-((x - X)**2) / (2 * h**2)))\n",
    "    y_prime = total / (n * h**3 * sqrt(2 * pi))\n",
    "    return(y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run this cell after you pass the validation tests above\n",
    "def plot_density_derivative(h):\n",
    "    x_values = np.arange(4, 36, 0.5)\n",
    "    y_values = [p_hat_derivative(x, h, X_observed) for x in x_values]\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    plt.plot(x_values, y_values)\n",
    "    plt.axhline(0, c = 'k', ls = \"--\")\n",
    "    plt.title(\"Derivative of the density $\\hat{p}_h(x)$\")\n",
    "    plt.xlabel(\"Velocity, x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe838e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactive plot: Do not modify\n",
    "interactive_plot = interactive(plot_density_derivative, h=(0.1, 4, 0.1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57367814",
   "metadata": {},
   "source": [
    "We notice that when $h$ is really small the derivative is very 'wiggly' and it frequently crosses the 0 line. As $h$ increases, it crosses the 0 line less frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f9295",
   "metadata": {},
   "source": [
    "## 3.b. Count the number of modes in $\\hat{p}_h(x)$\n",
    "\n",
    "Using the derivative calculated above, we will now count the number of modes in $\\hat{p}_h(x)$.\n",
    "\n",
    "To do this, we will evaluate the derivative $\\frac{d}{dx}\\hat{p}_h(x)$ at a grid of points $x_1,...,x_m$ evenly spaced between $5$ and $35$ (the lower and upper bounds on the velocities in the data), and count the number of times that the derivative crosses from positive to negative.  The use of a grid of $x$'s isn't a perfect measurement of the mode count, since if we don't evaluate the derivative at enough points that are close enough together, we may miss some modes. In this lab, we will make sure that the grid we use is fine enough to accurately count the number of modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the modes of phat using the derivative implemented above.\n",
    "def count_modes(x_values, h, X):  \n",
    "    \"\"\"Counts the number of modes in p_hat(x), approximated over the given grid of x_valies.\n",
    "    \n",
    "    Counts a mode every time the derivative of p_hat(x) crosses from positive (or 0)\n",
    "    to negative over the given grid of x_values.\n",
    "    \n",
    "    Inputs: \n",
    "      x_values : array of floats of length m \n",
    "      containing points at which to evaluate the derivative.\n",
    "      h: float, bandwidth parameter in phat_h.\n",
    "      X: array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      num_modes : int, the number of modes in p_hat(x).\n",
    "    \"\"\"\n",
    "    # First calculate the derivative at all points in x_values.\n",
    "    all_derivatives = [p_hat_derivative(x, h, X) for x in x_values]\n",
    "    \n",
    "    # Iterate through all of the calculated derivatives, \n",
    "    # and add a mode every time the derivative crosses from positive (or 0) to negative.\n",
    "    num_modes = 0\n",
    "    for i in range(0, len(all_derivatives)-1):\n",
    "        if (all_derivatives[i] >= 0) and (all_derivatives[i + 1] < 0):\n",
    "            num_modes += 1\n",
    "    \n",
    "    return num_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Plot the number of modes for different values of h.\n",
    "# This cell may take a few seconds to run.\n",
    "x_values = np.arange(5,35,0.05)\n",
    "h_values = np.arange(0.3,4,0.1)\n",
    "mode_counts = [count_modes(x_values, h, X_observed) for h in h_values]\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.plot(h_values, mode_counts)\n",
    "plt.title(\"Number of modes in $\\hat{p}_h(x)$\")\n",
    "plt.ylabel(\"Number of modes\")\n",
    "plt.xlabel(\"Bandwidth h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f590774",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.c. (To-do) Fill-in-the-blanks\n",
    "Based on your observation above, we can develop a stategy to find the number of modes. Fill in the blanks below.\n",
    "\n",
    "\"*To find the number of modes, we evaluate the (**Blank 1**)  on a grid of points, and count the number of times it goes from (**Blank 2**) to (**Blank 3**).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973315d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87989d5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 4. Hypothesis test\n",
    "\n",
    "Now that we've defined the density estimate $\\hat{p}_h(x)$ and figured out how to count the number of modes in $\\hat{p}_h(x)$, we will move on to testing whether or not a multimodal distribution can reasonably fit our data $X_1,...,X_n$.\n",
    "\n",
    "In the plot in part 2.b. you should have observed that the number of modes in $\\hat{p}_h(x)$ decreases monotonically as $h$ increases. Let $H_1$ be the minimal bandwidth value $h$ for which $\\hat{p}_h(x)$ is unimodal. \n",
    "\n",
    "\\begin{align}\n",
    "    H_1 & = \\min \\{h \\colon m(\\hat{p}_h) = 1, \\, m(\\hat{p}_{h'}) > 1 \\text{ for all } h' < h\\}.\n",
    "\\end{align}\n",
    "\n",
    "Similarly we define $H_k$ to be the minimal bandwidth value $h$ for which $\\hat{p}_h(k)$ has $k$ modes:\n",
    "\n",
    "\\begin{align}\n",
    "    H_k & = \\min \\{h \\colon m(\\hat{p}_h) = k, \\, m(\\hat{p}_{h'}) > k \\text{ for all } h' < h\\}.\n",
    "\\end{align}\n",
    "\n",
    "We will use $H_k$ as the test statistic. \n",
    "\n",
    "Notice that $H_k$ depends on the data $X$, because the function $\\hat{p}_h(x)$ depends on the data $X$.\n",
    "\n",
    "For our particular observed dataset $X_{observed}$, let $h_k$ be the observed minimal bandwidth value $h$ for which $\\hat{p}_h(x)$ has $k$ modes.\n",
    "\n",
    "\n",
    "## Calculate $H_k$\n",
    "The first thing we need to do is calculate $H_k$ for a given dataset $X$. To do this, we will try different values of $h$ until we find the smallest value such that the density estimate $\\hat{p}_h$ has $k$ modes. The function below accomplishes that. Take a few minutes to examine it and understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfaf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just understand what this function is doing.\n",
    "def find_hk(x_values, X, h_min=0.3, h_max=4, h_err = 0.01, k=1):\n",
    "    \"\"\"\n",
    "    Calculates h_k, the minimum bandwidth h such that the density estimate p_hat has k modes.\n",
    "    Chooses h_k from within an interval bounded by h_min and h_max, within error h_err.\n",
    "    \n",
    "    Inputs:\n",
    "      x_values: array of floats containing points x to use to count the number of modes in p_hat.\n",
    "      X: array of floats of length n containing the observed galaxy velocities.\n",
    "      h_min: float, minimum h to try.\n",
    "      h_max: float, maximum h to try.\n",
    "      h_err: float, allowed error of h, or step size of hs to try between h_min and h_max.\n",
    "      k: number of modes being tested in the hypothesis test.\n",
    "      \n",
    "    Returns:\n",
    "      h_k: minimum bandwith h among candidate h_values such that p_hat has k modes.\n",
    "    \"\"\"\n",
    "    # Perform a binary search to find the minimum bandwith hk.\n",
    "    h_opt = 0\n",
    "    modes_min = count_modes(x_values, h_min, X)\n",
    "    modes_max = count_modes(x_values, h_max, X)\n",
    "    while h_max - h_min > h_err:\n",
    "        h_opt = (h_min + h_max) / 2\n",
    "        modes_opt = count_modes(x_values, h_opt, X)\n",
    "        if modes_opt > k:\n",
    "            h_min = h_opt\n",
    "            modes_min = modes_opt\n",
    "        else:\n",
    "            h_max = h_opt\n",
    "            modes_max = modes_opt\n",
    "    return h_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58bc0e",
   "metadata": {},
   "source": [
    "The function above calculates the test statistic $H_k$. To calculate the value $h_1$ for the null hypothesis, we apply this same function over the observed data $X_1,...,X_n$. Run the cell below and compare the outputs with the plot in 2.b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c56985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to calculate the value of h_k using the function above.\n",
    "# This cell might take a minute or so to run\n",
    "x_values = np.arange(5,35,0.05)\n",
    "for k in range(1,10):\n",
    "    hk = find_hk(x_values, X_observed, k=k)\n",
    "    print(\"For k = {}. Estimate value of h_{}: {:.4f}\".format(k, k, hk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f00ec",
   "metadata": {},
   "source": [
    "##  Computing the $p$-value\n",
    "\n",
    "Let's say we are trying to test if the distribution of galaxies' velocities is unimodal. The corresponding test statistic is $H_1$ and the observed realization is $h_1$. Therefore, the $p$-value for our hypothesis test is: \n",
    "\n",
    "$$P_{0}(H_1 \\geq h_1)$$\n",
    "\n",
    "where $P_0$ is the probability under the null hypothesis that the $X_i$ are drawn from a unimodal distribution. This $p$-value represents the probability under the null hypothesis that we observe a value as extreme as $h_1$ for the minimum width parameter.\n",
    "\n",
    "To perform a hypothesis test at significance level $\\alpha$, we reject the null hypothesis if the $p$-value is less than $\\alpha$:\n",
    "$$P_{0}(H_1 \\geq h_1) \\leq \\alpha. $$\n",
    "\n",
    "Now, we need to calculate the $p$-value. Unfortunately, we don't have a closed form for the distribution of the test statistic $H_1$ under the null hypothesis that the $X_i$ are drawn from a unimodal distribution. In fact, we don't even know what distribution the $X_i$ are drawn from, only that it's unimodal (under the null hypothesis)! Still, to estimate the distribution of the test statistic $H_1$, we need to pick some distribution to use for the distribution of the $X_i$'s under the null hypothesis.\n",
    "\n",
    "Among the parameterized densities $\\hat{p}_h(x)$, the density $\\hat{p}_{h_1}(x)$ is the closest unimodal distribution  to the empirical distribution $p$ of the observed data. So, we will use $\\hat{p}_{h_1}(x)$ as the distribution of the $X_i$'s under the null hypothesis.\n",
    "\n",
    "Therefore, the $p$-value that we will calculate is $$P_{X_i \\sim \\hat{p}_{h_1}}(H_1 \\geq h_1).$$\n",
    "\n",
    "More generally, if instead we want to test if the distribution of galaxies' velocities has at most $k$ modes can be calculated as:\n",
    "$$P_{0}(H_k \\geq h_k) \\approx P_{X_i \\sim \\hat{p}_{h_k}}(H_k \\geq h_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f776aa5",
   "metadata": {},
   "source": [
    "##  4.a. Sampling from $\\hat{p}_{h_k}$ using the Bootstrap Method\n",
    "\n",
    "To calculate the $p$-value, we will first draw i.i.d. samples from $\\hat{p}_{h_k}$, and then observe the number of times that the $H_k$ calculated from those samples is greater than or equal to $h_k$. We will use the bootstrap to draw the i.i.d. samples from $\\hat{p}_{h_k}$.\n",
    "\n",
    "Let $Z^{*} = (Z_1^*, \\ldots, Z_{82}^*)$ denote a bootstrap sample from the dataset $X_{observed}$. It can be shown that by adding some scaled noise to the bootstrap samples we can obtain samples from the null distribution. More precisely: \n",
    "$Z_i^* + h_k \\epsilon_i$ for $\\epsilon_i \\sim \\mathcal{N}(0, 1)$ gives i.i.d. samples from $\\hat{p}_{h_k}$.\n",
    "\n",
    "This leads to the following bootstrap algorithm: \n",
    "\n",
    "1. Draw $B$ independent bootstrap samples $Z^{*(1)}, \\ldots, Z^{*(B)}$ from the observed data. Then add some noise to get samples  $X^{*(1)}, \\ldots, X^{*(B)}$ from the null distribution $\\hat{p}_{h_1}$:\n",
    "    \\begin{align}\n",
    "        X_i^{*(b)} & = Z_i^{*(b)} + h_k \\epsilon_i^{(b)} \\\\\n",
    "        \\epsilon_i^{(b)} & \\sim \\mathcal{N}(0, 1)\n",
    "    \\end{align}\n",
    "    \n",
    "    However, since the variance of the bootstrap sample has been increased by adding the normal error term, the data are usually rescaled to have the same sample variance as the original observations. So, we replace the equation above in the algorithm with \n",
    "    \n",
    "    \\begin{align}\n",
    "        X_i^{*(b)} & = \\bar{Z}^{*(b)} + (1 + h_k^2/\\hat{\\sigma}^2)^{-1/2} (Z_i^{*(b)} - \\bar{Z}^{*(b)} + h_k\\epsilon_i^{(b)}).\n",
    "    \\end{align}\n",
    "    \n",
    "    We'll use this more complicated variance scaling in the code. In the equation above we have:\n",
    "    - $\\bar{Z}^{*(b)}$ is the sample mean of the bootstrap samples $Z^{*(b)}$.\n",
    "    - $\\hat\\sigma$ is the variance of the original observed data.\n",
    "    - $h_k$ is the minimum bandwidth h such that the density estimate for the original observed data has k modes.\n",
    "    - $\\epsilon_i^{(b)} \\sim \\mathcal{N}(0, 1)$, iid Gaussian noise\n",
    "    \n",
    "   We will call $ X_i^{*(b)}$ **bootstrap replicates**. \n",
    "       \n",
    "       \n",
    "       \n",
    "2. For each bootstrap replicate $X^{*{b}}$, evaluate the value of the test statistic $H_k^{*(b)}$.\n",
    "    \n",
    "3. Estimate the $p$-value as the fraction of time that the value of the test statistic evaluated on the bootstrap replicates is larger the the test statistic evaluated on the original observed data.\n",
    "    \\begin{align}\n",
    "        \\text{estimate of }  \\mathbb{P}_0(H_k \\geq h_k) = \\frac{1}{B} \\sum_{b = 1}^B 1[H_k^{*(b)} \\geq h_k].\n",
    "    \\end{align}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b93bd",
   "metadata": {},
   "source": [
    "First, you'll write some code to draw bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef3480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_Z_star(X):\n",
    "    \"\"\"Resample (with replacement) a new array the same length as X \n",
    "    \n",
    "    Inputs: \n",
    "      X: array of floats containing the observed galaxy velocities\n",
    "    \n",
    "    Outputs:\n",
    "      Z_star: array of floats resampled from X with replacement\n",
    "      Z_bar: the mean of Z_star\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: obtain the bootstrap sample Z_star, and its mean Z_bar\n",
    "    # Z_star should be an array of n samples drawn from the data array X, sampled with replacement.\n",
    "    # Hint: use np.random.choice.\n",
    "    Z_star = ...\n",
    "    Z_bar = ...\n",
    "    return Z_star, Z_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35221c82",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de75b95",
   "metadata": {},
   "source": [
    "Then, we will calculate the p-values using these bootstrap samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ea9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here\n",
    "def estimate_p_value(X, B, k=1):\n",
    "    \"\"\"Estimates the p-value for the hypothesis test.\n",
    "    \n",
    "    Inputs: \n",
    "      X: array of floats containing the observed galaxy velocities.\n",
    "      B: int, number of bootstrap samples to draw.\n",
    "      k: int, number of modes we are testing for.\n",
    "    \n",
    "    Outputs:\n",
    "      p_value: float, an estimate of the p-value.\n",
    "    \"\"\"\n",
    "    # Find hk for the distribution under the null hypothesis.\n",
    "    x_values = np.arange(5,35,0.05)\n",
    "    hk = find_hk(x_values, X, k=k)\n",
    "    # Count of the number of times Hk >= hk.\n",
    "    Hk_greater_count = 0\n",
    "    # Variance of the observed data X for rescaling the data.\n",
    "    X_var = np.var(X)\n",
    "    n = len(X)\n",
    "    for _ in range(B):\n",
    "        Z_star, Z_bar = draw_Z_star(X)\n",
    "        \n",
    "        epsilon = np.random.normal(size=n)\n",
    "        X_star = Z_bar + (1 / sqrt(1 + ((hk**2) / X_var))) * (Z_star - Z_bar + (hk * epsilon)) # TODO: fill in\n",
    "        \n",
    "        # Check if H1 >= h1. Instead of explicitly calculating H1 (which could take long), \n",
    "        # we are using a shortcut where we count the number of modes in X_star under bandwidth value h1.\n",
    "        # If the counted number of modes is greater than the number of modes used to find h1 \n",
    "        # for the observed data, then the bandwidth value H1 is greater than or equal to the bandwidth value h1.\n",
    "        # This is true because of number of modes is monotonically decreasing in the bandwidth value h.\n",
    "        modes = count_modes(x_values, hk, X_star)\n",
    "        if modes > k:\n",
    "            Hk_greater_count += 1\n",
    "    p_value = Hk_greater_count / B \n",
    "    return(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03657391",
   "metadata": {},
   "source": [
    "## Try testing for different numbers of modes.\n",
    "\n",
    "If we reject the hypothesis that the distribution of the data has 1 mode, what about testing if the distribution has more than $k$ modes? We can apply the same techniques to test \n",
    "\n",
    "$$H_0: m(p) = k$$ \n",
    "$$H_A: m(p) > k$$ \n",
    "\n",
    "Below, we apply the same techniques to estimate the $p$-values for $k = 2$ and $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value. \n",
    "p_val_1 = estimate_p_value(X_observed, 100, k=1)\n",
    "print(\"p-value for test for more than 1 mode:\", p_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value.\n",
    "# k = 2\n",
    "p_val_2 = estimate_p_value(X_observed, 100, k=2)\n",
    "print(\"p-value for test for more than 2 modes:\", p_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf20ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value.\n",
    "# k = 3\n",
    "p_val_3 = estimate_p_value(X_observed, 100, k=3)\n",
    "print(\"p-value for test for more than 3 modes:\", p_val_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fbbb3",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.b. For which values of $k$ were you able to reject the null hypothesis? Did this match your expectation of the number of modes in the data based on looking at the initial histogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179aa8aa",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae306c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d19665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab06.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab06.pdf')):\n",
    "    img = mpimg.imread('chinchilla.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab06.pdf' download>here</a>.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05221c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c10893",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00076392",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q0c_i": {
     "name": "q0c_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert q0c_i.upper() == \"B\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q0c_iii": {
     "name": "q0c_iii",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> first_q0_10 = np.array([2.31040879, 2.28676486, 2.21883405, 2.21736877, 2.22888946,\n...        2.21329085, 2.23160438, 2.24657932, 2.27997392, 2.26348672])\n>>> assert np.allclose(trace.posterior.q0[:, :].to_numpy().flatten()[:10], first_q0_10)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> first_q1_10 = np.array([0.48528855, 0.45162568, 0.58493267, 0.56287812, 0.59027936,\n...        0.54309099, 0.42550684, 0.4674696 , 0.46644434, 0.55409565])\n>>> assert np.allclose(trace.posterior.q1[:, :].to_numpy().flatten()[:10], first_q1_10)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q0d": {
     "name": "q0d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bambi_ta_10 = np.array([0.4844136 , 0.4844136 , 0.52475228, 0.5088275 , 0.5088275 ,\n...        0.37721286, 0.44015103, 0.50478428, 0.40513035, 0.43697324])\n>>> \n>>> assert np.allclose(my_model_samples.posterior.Temp_Anomaly.to_numpy().flatten()[:10], bambi_ta_10)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bambi_int_10 = np.array([2.20638771, 2.20638771, 2.26402576, 2.2418237 , 2.2418237 ,\n...        2.21996355, 2.26259078, 2.22356198, 2.23562492, 2.27393475])\n>>> \n>>> assert np.allclose(my_model_samples.posterior.Intercept.to_numpy().flatten()[:10], bambi_int_10)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x_values = [10, 15, 20, 25, 30, 35]\n>>> h_values = [0.5, 1, 2, 5, 10]\n>>> inputs = list(itertools.product(x_values, h_values))\n>>> solns = [0.043502122432280646,\n...  0.030026013640726264,\n...  0.016530138724089653,\n...  0.013967855793789412,\n...  0.021926134614069594,\n...  0.0015575391599632143,\n...  0.00519520776222485,\n...  0.010784571470806413,\n...  0.036346588784896126,\n...  0.03140794584332124,\n...  0.19868844845841,\n...  0.15019369808301325,\n...  0.11000604476729897,\n...  0.063136488323517,\n...  0.03653220304335537,\n...  0.037353630386100754,\n...  0.047323200324462446,\n...  0.05568273959712373,\n...  0.05225135965094527,\n...  0.03426463219010833,\n...  1.9261240885591453e-06,\n...  0.0007509388899324225,\n...  0.0047072196317858896,\n...  0.020949634343078616,\n...  0.025849865901999173,\n...  0.003440866180505044,\n...  0.004239372667838794,\n...  0.004429928242457236,\n...  0.005609566372465535,\n...  0.015716907454831598]\n>>> i = 0\n>>> for x in x_values:\n...     for h in h_values:\n...         assert np.isclose(p_hat(x, h, X_observed), solns[i])\n...         i += 1\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.random.seed(13)\n>>> Z_star, Z_bar = draw_Z_star(X_observed)\n>>> assert Z_star[0] == 21.921\n>>> assert np.isclose(Z_bar, 20.563743902439025)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
